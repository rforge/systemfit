%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other issues and tools}\label{sec:Other}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Residual variance-covariance matrix}\label{sec:residcov}

Since the true residuals of the estimated equations are generally not known,
the true covariance matrix of the residuals cannot be determined.
Thus, this covariance matrix must be calculated from the
\emph{estimated} residuals. 

The estimated covariance matrix of the residuals ($\widehat{\Sigma} = 
\left[ \widehat{\sigma}_{ij} \right]$) 
is often calculated by
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }{ T }
   \label{eq:rcov0}
\end{equation}
where $T$ is the number of observations in each equation.

However, this estimator is biased, because it is not corrected for 
degrees of freedom. 
The usual single-equation procedure to correct for degrees of freedom 
cannot always be applied, because the number of regressors in each equation 
might differ.
Two alternative approaches to calculate the residual variance-covariance 
matrix are
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { \sqrt{ \left( T - K_i \right) \cdot \left( T - K_j \right) } }
   \label{eq:rcov1}
\end{equation}
and
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { T - \max \left( K_i , K_j \right) }
\end{equation}
where $K_i$ and $K_j$ are the number of regressors in equation
$i$ and $j$, respectively.

However, this yields an unbiased estimator only if $K_i = K_j$ 
\citep[p. 469]{judge85}. 
% Greene (2002, p. 344) says that the second is unbiased if i = j or K_i = K_j,
% whereas the first is unbiased only if i = j. 
% However, if K_i = K_j the first and the second are equal.
% Why is the first biased if K_i = K_j ???????????


A further approach to obtain the estimated residual variance-covariance
matrix is \citep[p. 309]{zellner62c}
\begin{eqnarray}
   \widehat{\sigma}_{ij} & = & 
   \frac{ \widehat{u}_i' \widehat{u}_j } 
   { T - K_i - K_j + tr \left[ X_i \left( X_i' X_i \right)^{-1}
   X_i' X_j \left( X_j' X_j \right)^{-1} X_j' \right] } \\
   & = &
   \frac{ \widehat{u}_i' \widehat{u}_j } 
   { T - K_i - K_j + tr \left[ \left( X_i' X_i \right)^{-1}
   X_i' X_j \left( X_j' X_j \right)^{-1} X_j' X_i \right] } \\
   \label{eq:rcov2}
\end{eqnarray} 
This yields an unbiased estimator for all elements of $\widehat{\Sigma}$,
but even if $\widehat{\Sigma}$ is an unbiased estimator of $\Sigma$, 
its inverse $\widehat{\Sigma}^{-1}$ is not an unbiased estimator 
of $\Sigma^{-1}$ \citep[p.322]{theil71}.
Furthermore, the variance-covariance matrix calculated by (\ref{eq:rcov2})
is not necessarily positive semidefinite \citep[p.322]{theil71}. 
Hence, "it is doubtful whether [this formula] is really superior to 
[(\ref{eq:rcov0})]" \citep[p.322]{theil71}.


The WLS, SUR, W2SLS and 3SLS parameter estimates are consistent,
if the estimated residual variance-covariance matrix is calculated
using the residuals from a first step OLS or 2SLS estimation.
It is also possible to iterate the estimation procedure and 
calculate the residual variance-covariance matrix from the
residuals of the previous iteration. 
If equation (\ref{eq:rcov0}) is applied to calculate the estimated
residual variance-covariance matrix, the results are maximum
likelihood \citep[p. 345]{greene02}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Degrees of freedom}

There exist different approaches to determine the degrees of
freedom for testing the estimated parameters with a t-test.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Goodness of fit}

The goodness of fit of each equation can be measured by the 
traditional $R^2$ values:
\begin{equation}
   R_i^2 = 1 - \frac{ \widehat{u}_i' \widehat{u}_i }
   { ( y_i - \overline{y_i} )' ( y_i - \overline{y_i} ) }
\end{equation}
where $R_i^2$ is the $R^2$ value of the $i$th equation
and $\overline{y_i}$ is the mean value of $y_i$.

The goodness of fit of the whole system can be measured by the
McElroy's $R^2$ value \citep{mcelroy77}: 
% also: \citep[p. 345]{greene02}
\begin{equation}
   R_*^2 = 1 - \frac{ \widehat{u}' \widehat{ \Omega }^{-1} \widehat{u} }
   { y' \left( \widehat{ \Sigma }^{-1} \otimes
   \left( I - \frac{i i'}{T} \right) \right) y }
\end{equation}
where $T$ is the number of observations in each equation,
$I$ is an $T \times T$ identity matrix and 
$i$ is a column vector of $T$ ones.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood ratio test}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hausman test}

\textbf{Man, I really have to clean this up.}

Hausman \citep{hausman1978} developed a test have power against
alternatives for which $\hat{\theta}$ and $\tilde{\theta}$ diverge
under misspecification.

The Hausman result if the difference between the covariance of an
efficient and the covariance of the inefficient estimator is zero, the
model is correctly specified.  

The test is based on the idea that under the hypothesis of no
correlation, both the limited information estimates and the
full-information results are consistent, but only the full-information
estimates are efficient and under the alternative, the limited
information estimates are consistent, but the full information
estimates are not. 

Since the models that are fit using the \pkg{systemfit} package are
typically random effects models, the effects may be inconsistent
between the introduced variables and the regressors.

