
%       $Id$    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Other issues and tools}\label{sec:Other}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Residual covariance matrix}\label{sec:residcov}

Since the (true) disturbances ($u_i$) of the estimated equations
are generally not known,
their covariance matrix cannot be determined.
Therefore, this covariance matrix is generally calculated from
estimated residuals ($\uHat_i$)
that are obtained from a first-step OLS or 2SLS estimation.
Then, in a second step, the estimated residual covariance matrix
can be employed for a WLS, SUR, W2SLS, or 3SLS estimation.
In many cases, the residual covariance matrix is calculated by
\begin{equation}
   \sHat_{ij} = \frac{ \uHat_i^\top \uHat_j }{ T },
   \label{eq:rcovNoDfCor}
\end{equation}
where $T$ is the number of observations in each equation.
However, in finite samples this estimator is biased,
because it is not corrected for degrees of freedom.
The usual single-equation procedure to correct for degrees of freedom
cannot always be applied, because the number of regressors in each equation
might differ.
Two alternative approaches to calculate the residual covariance
matrix are
\begin{equation}
   \sHat_{ij} = \frac{ \uHat_i^\top \uHat_j }
   { \sqrt{ \left( T - K_i \right) \cdot \left( T - K_j \right) } }
   \label{eq:rcovGeomean}
\end{equation}
and
\begin{equation}
   \sHat_{ij} = \frac{ \uHat_i^\top \uHat_j }
   { T - \max \left( K_i , K_j \right) } \; ,
   \label{eq:rcovMax}
\end{equation}
where $K_i$ and $K_j$ are the number of regressors in equation
$i$ and $j$, respectively.
However, these formulas yield unbiased estimators only if $K_i = K_j$
\citep[p.~469]{judge85}.
% Greene (2003, p. 344) says that the second is unbiased if i = j or K_i = K_j,
% whereas the first is unbiased only if i = j. 
% However, if K_i = K_j the first and the second are equal.
% Why is the first biased if K_i = K_j ???????????


A further approach to obtain a residual covariance
matrix is
\begin{eqnarray}
   \sHat_{ij} & = &
   \frac{ \uHat_i^\top \uHat_j }
   { T - K_i - K_j + \tr \left[ X_i \left( X_i^\top X_i \right)^{-1}
   X_i^\top X_j \left( X_j^\top X_j \right)^{-1} X_j^\top \right] }
   \label{eq:rcovTheil} \\
   & = &
   \frac{ \uHat_i^\top \uHat_j }
   { T - K_i - K_j + \tr \left[ \left( X_i^\top X_i \right)^{-1}
   X_i^\top X_j \left( X_j^\top X_j \right)^{-1} X_j^\top X_i \right] }
\end{eqnarray} 
\citep[p.~309]{zellner62c}.
This yields an unbiased estimator for all elements of $\Sigma$,
but even if $\SHat$ is an unbiased estimator of $\Sigma$,
its inverse $\SHat^{-1}$ is not an unbiased estimator
of $\Sigma^{-1}$ \citep[p.~322]{theil71}.
Furthermore, the covariance matrix calculated by Equation~\ref{eq:rcovTheil}
is not necessarily positive semidefinite \citep[p.~322]{theil71}.
Hence, °it is doubtful whether [this formula] is really superior to 
[Equation~\ref{eq:rcovNoDfCor}]° \citep[p.~322]{theil71}.


The WLS, SUR, W2SLS and 3SLS coefficient estimates are consistent
if the residual covariance matrix is calculated
using the residuals from a first-step OLS or 2SLS estimation.
There exists also an alternative slightly different approach
that consists of three steps.%
\footnote{
For instance, this approach is applied by
the command °TSCS° of the software \proglang{LIMDEP} that carries out SUR estimations
in which all coefficient vectors are constrained to be equal
\citep{greene06}.
}
In a first step, an OLS or 2SLS estimation is applied to obtain residuals
to calculate a (first-step) residual covariance matrix.
In a second step, the first-step residual covariance matrix
is used to estimate the model by WLS or W2SLS
and new residuals are obtained to calculate
a (second-step) residual covariance matrix.
Finally, in the third step,
the second-step residual covariance matrix is used
to estimate the model by SUR or 3SLS.
If the estimated coefficients are not constrained by cross-equation restrictions,
OLS and WLS estimates as well as 2SLS and W2SLS estimates are identical.
Hence, in this case both approaches generate the same results.

It is also possible to iterate WLS, SUR, W2SLS and 3SLS estimations.
At each iteration the residual covariance matrix is calculated
from the residuals of the previous iteration.
If Equation~\ref{eq:rcovNoDfCor} is applied to calculate the
residual covariance matrix,
an iterated SUR estimation converges to maximum
likelihood \citep[p.~345]{greene03}.

In some uncommon cases,
for instance in pooled estimations,
where the coefficients are restricted to be equal in all equations,
the means of the residuals of each equation are not equal to zero
$( \overline{ \uHat }_i \neq 0 )$.
Therefore, it might be argued
that the residual covariance matrix should be calculated
by subtracting the means from the residuals
and substituting $\uHat_i - \overline{ \uHat }_i$
for $\uHat_i$ in Equations~\ref{eq:rcovNoDfCor}--\ref{eq:rcovTheil}.

If the coefficients are estimated under any restrictions,
the residual covariance matrix
for a WLS, SUR, W2SLS, or 3SLS estimation
can be obtained either from a restricted or from an unrestricted
first-step estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Degrees of freedom}
\label{sec:degreesOfFreedom}

To our knowledge the question about how to determine the degrees
of freedom for single-coefficient $t$ tests is not comprehensively
discussed in the literature.
While sometimes the degrees of freedom of the entire system
(total number of observations in all equations minus
total number of estimated coefficients)
are applied,
in other cases the degrees of freedom of each single equation
(number of observations in the equations minus
number of estimated coefficients in the equation)
are used.
Asymptotically, this distinction does not make a difference.
However, in many empirical applications, the number of observations
of each equation is rather small, and
therefore, it matters.

If a system of equations is estimated by an unrestricted OLS and
the covariance matrix of the coefficients is calculated
with $\OHat$ in Equation~\ref{eq:cov-ols-wls-sur}
equal to $\SHat \otimes I_T$,
the estimated coefficients and their standard errors are identical
to an equation-wise OLS estimation.
In this case, it is reasonable to use the degrees of freedom of
each single equation,
because this yields the same $P$ values as the equation-wise
OLS estimation.

In contrast, if a system of equations is estimated with many
cross-equation restrictions and
the covariance matrix of an OLS estimation is calculated
with $\OHat$ in Equation~\ref{eq:cov-ols-wls-sur}
equal to $\sHat^2 I_{G \cdot T}$,
the system estimation is similar to a single equation estimation.
Therefore, in this case, it seems to be reasonable to use the degrees
of freedom of the entire system.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Goodness of fit}

The goodness of fit of each single equation can be measured by the
traditional $R^2$ values
\begin{equation}
   R_i^2 = 1 - \frac{ \uHat_i^\top \uHat_i }
   { ( y_i - \overline{y_i} )^\top ( y_i - \overline{y_i} ) } \; ,
\end{equation}
where $R_i^2$ is the $R^2$ value of the $i$th equation
and $\overline{y_i}$ is the mean value of $y_i$.

The goodness of fit of the whole system can be measured by the
McElroy's $R^2$ value 
% also: \citep[p.~345]{greene03}
\begin{equation}
   R_*^2 = 1 - \frac{ \uHat^\top \OHat^{-1} \uHat }
   { y^\top \left( \SHat^{-1} \otimes
   \left( I_T - \frac{\iota \iota^\top}{T} \right) \right) y } ,
\end{equation}
where $\iota$ is a column vector of $T$ ones \citep{mcelroy77}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testing linear restrictions}
\label{sec:testingRestrictions}

Linear restrictions can be tested by an $F$ test, two Wald tests and
a likelihood ratio (LR) test.

The $F$ statistic for systems of equations is
\begin{equation}
F = \frac{
   ( R \bHat - q )^\top
   ( R ( X^\top ( \Sigma \otimes I )^{-1} X )^{-1} R^\top )^{-1}
   ( R \bHat - q ) /
   j
}{
   \uHat^\top ( \Sigma \otimes I )^{-1} \uHat /
   ( G \cdot T - K )
} \; ,
\label{eq:f-test}
\end{equation}
where $j$ is the number of restrictions,
$K$ is the total number of estimated coefficients,
and all other variables are as defined before
\citep[p.~314]{theil71}.
Under the null hypothesis, $F$ is $F$ distributed
with $j$ and $G \cdot T - K$ degrees of freedom.

However, $F$ in Equation~\ref{eq:f-test} cannot be computed,
because $\Sigma$ is generally unknown.
As a solution, \citet[p.~314]{theil71} proposes
to replace the unknown $\Sigma$ in Equation~\ref{eq:f-test}
by the estimated covariance matrix $\SHat$.
\begin{equation}
\hat{F} = \frac{
   ( R \bHat - q )^\top
   ( R ( X^\top ( \SHat \otimes I )^{-1} X )^{-1} R^\top )^{-1}
   ( R \bHat - q ) /
   j
}{
   \uHat^\top ( \SHat \otimes I )^{-1} \uHat /
   ( G \cdot T - K )
}
\label{eq:f-test-theil}
\end{equation}
Asymptotically, $\hat{F}$ has the same distribution as $F$
in Equation~\ref{eq:f-test},
because the numerator of Equation~\ref{eq:f-test-theil}
converges in probability to the numerator of Equation~\ref{eq:f-test}
and the denominator of Equation~\ref{eq:f-test-theil}
converges in probability to the denominator of Equation~\ref{eq:f-test}
\citep[p.~402]{theil71}.

Furthermore, the denominators of both Equations~\ref{eq:f-test}
and~\ref{eq:f-test-theil} converge in probability to~$1$.
Taking this into account and applying Equation~\ref{eq:cov-ols-wls-sur},
we obtain the usual $F$~statistic of the Wald test.
\begin{equation}
\hat{\hat{F}} = \frac{
   ( R \bHat - q )^\top
   ( R \, \COVHat \left[ \bHat \right] R^\top )^{-1}
   ( R \bHat - q )
}{
   j
}
\label{eq:f-test-wald}
\end{equation}
Under the null hypotheses, also $\hat{\hat{F}}$ is
asymptotically $F$ distributed
with $j$ and $G \cdot T - K$ degrees of freedom.

Multiplying Equation~\ref{eq:f-test-wald} with $j$,
we obtain the usual $\chi^2$ statistic for the Wald test
\begin{equation}
W =
   ( R \bHat - q )^\top
   ( R \, \COVHat [ \bHat ] \, R^\top )^{-1}
   ( R \bHat - q ) .
\label{eq:chi2-test-wald}
\end{equation}
Asymptotically, $W$ has a $\chi^2$
distribution with $j$ degrees of freedom
under the null hypothesis
\citep[p.~347]{greene03}.

The likelihood-ratio (LR) statistic for systems of equations is
\begin{equation}
\LR = T \cdot \left(
   \log \left| \SHat_r \right|
   - \log \left| \SHat_u \right|
   \right) ,
\label{eq:lr-test}
\end{equation}
where $T$ is the number of observations per equation, and 
$\SHat_r$ and $\SHat_u$ are
the residual covariance matrices calculated by Equation~\ref{eq:rcovNoDfCor}
of the restricted and unrestricted estimation, respectively.
Asymptotically, $\LR$ has a $\chi^2$
distribution with $j$ degrees of freedom
under the null hypothesis
\citep[p.~349]{greene03}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hausman test}
\label{sec:hausman}

\citet{hausman78} developed a test for misspecification.
The null hypothesis of the test is
that the instrumental variables of each equation are
uncorrelated with the disturbance terms of all other equations
($\E \left[ u_i^\top Z_j \right] = 0 \, \forall \, i \neq j$).
Under this null hypothesis, both the 2SLS and the 3SLS estimator are consistent,
but the 3SLS estimator is (asymptotically) more efficient.
Under the alternative hypothesis, the 2SLS estimator is consistent
but the 3SLS estimator is inconsistent,
i.e.\ the instrumental variables of each equation are
uncorrelated with the disturbances of the same equation
($\E \left[ u_i^\top Z_i \right] = 0 \, \forall \, i$),
but the instrumental variables of at least one equation
are correlated with the disturbances of another equation
($\E \left[ u_i^\top Z_j \right] \neq 0 \, \exists \, i \neq j$).
The Hausman test statistic is
\begin{equation}
  m = \left( \bHat_\text{2SLS} - \bHat_\text{3SLS} \right)^\top
      \left( \COVHat \left[ \bHat_\text{2SLS} \right] -
             \COVHat \left[ \bHat_\text{3SLS} \right] \right)
      \left( \bHat_\text{2SLS} - \bHat_\text{3SLS} \right) ,
\label{eq:hausman}
\end{equation}
where $\bHat_\text{2SLS}$ and $\COVHat \left[ \bHat_\text{2SLS} \right]$ are the estimated
coefficient and covariance matrix from a 2SLS estimation, and
$\bHat_\text{3SLS}$ and $\COVHat \left[ \bHat_\text{3SLS} \right]$ are the estimated
coefficients and covariance matrix from a 3SLS estimation.
Under the null hypothesis, this test statistic has a
$\chi^2$ distribution with degrees of freedom equal to the number of
estimated coefficients.




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "systemfit"
%%% End: 
