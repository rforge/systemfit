
%       $Id$    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other issues and tools}\label{sec:Other}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Residual variance-covariance matrix}\label{sec:residcov}

Since the true residuals of the estimated equations are generally not known,
the true covariance matrix of the residuals cannot be determined.
Thus, this covariance matrix must be calculated from the
\emph{estimated} residuals. 

The estimated covariance matrix of the residuals ($\widehat{\Sigma} = 
\left[ \widehat{\sigma}_{ij} \right]$) 
is often calculated by
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }{ T }
   \label{eq:rcov0}
\end{equation}
where $T$ is the number of observations in each equation.

However, this estimator is biased, because it is not corrected for
degrees of freedom.
The usual single-equation procedure to correct for degrees of freedom
cannot always be applied, because the number of regressors in each equation
might differ.
Two alternative approaches to calculate the residual variance-covariance
matrix are
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { \sqrt{ \left( T - K_i \right) \cdot \left( T - K_j \right) } }
   \label{eq:rcov1}
\end{equation}
and
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { T - \max \left( K_i , K_j \right) }
\end{equation}
where $K_i$ and $K_j$ are the number of regressors in equation
$i$ and $j$, respectively.

However, this yields an unbiased estimator only if $K_i = K_j$ 
\citep[p.\ 469]{judge85}. 
% Greene (2002, p. 344) says that the second is unbiased if i = j or K_i = K_j,
% whereas the first is unbiased only if i = j. 
% However, if K_i = K_j the first and the second are equal.
% Why is the first biased if K_i = K_j ???????????


A further approach to obtain the estimated residual variance-covariance
matrix is \citep[p.\ 309]{zellner62c}
\begin{eqnarray}
   \widehat{\sigma}_{ij} & = & 
   \frac{ \widehat{u}_i' \widehat{u}_j } 
   { T - K_i - K_j + tr \left[ X_i \left( X_i' X_i \right)^{-1}
   X_i' X_j \left( X_j' X_j \right)^{-1} X_j' \right] } \\
   & = &
   \frac{ \widehat{u}_i' \widehat{u}_j } 
   { T - K_i - K_j + tr \left[ \left( X_i' X_i \right)^{-1}
   X_i' X_j \left( X_j' X_j \right)^{-1} X_j' X_i \right] } \\
   \label{eq:rcov2}
\end{eqnarray} 
This yields an unbiased estimator for all elements of $\widehat{\Sigma}$,
but even if $\widehat{\Sigma}$ is an unbiased estimator of $\Sigma$, 
its inverse $\widehat{\Sigma}^{-1}$ is not an unbiased estimator 
of $\Sigma^{-1}$ \citep[p.\ 322]{theil71}.
Furthermore, the variance-covariance matrix calculated by (\ref{eq:rcov2})
is not necessarily positive semidefinite \citep[p.\ 322]{theil71}. 
Hence, °it is doubtful whether [this formula] is really superior to 
[(\ref{eq:rcov0})]° \citep[p.\ 322]{theil71}.


The WLS, SUR, W2SLS and 3SLS parameter estimates are consistent,
if the estimated residual variance-covariance matrix is calculated
using the residuals from a first step OLS or 2SLS estimation.
It is also possible to iterate the estimation procedure and 
calculate the residual variance-covariance matrix from the
residuals of the previous iteration. 
If equation (\ref{eq:rcov0}) is applied to calculate the estimated
residual variance-covariance matrix, the results are maximum
likelihood \citep[p.\ 345]{greene02}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Degrees of freedom}

Don't we already mention this in the usage section.

Different approaches to determine the degrees of
freedom for testing the estimated parameters with a t-test
\textbf{(cite this one)}.

\textbf{Arne, do you have a citation for this?}

\paragraph{Degrees of freedom for t-tests}   
\code{probdfsys} use the degrees of freedom of the whole system
   (in place of the degrees of freedom of the single equation)
   to calculate prob values for the t-test of individual parameters.

The \code{probdfsys} can be used to set the degrees of freedom when
calculating the probability values for the t-test for individual
parameters.
This argument can set the degrees of freedom for the entire system
instead of using the degrees of freedom for each of the
individual equations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Goodness of fit}

The goodness of fit of each equation can be measured by the 
traditional $R^2$ values:
\begin{equation}
   R_i^2 = 1 - \frac{ \widehat{u}_i' \widehat{u}_i }
   { ( y_i - \overline{y_i} )' ( y_i - \overline{y_i} ) }
\end{equation}
where $R_i^2$ is the $R^2$ value of the $i$th equation
and $\overline{y_i}$ is the mean value of $y_i$.

The goodness of fit of the whole system can be measured by the
McElroy's $R^2$ value \citep{mcelroy77}: 
% also: \citep[p.\ 345]{greene02}
\begin{equation}
   R_*^2 = 1 - \frac{ \widehat{u}' \widehat{ \Omega }^{-1} \widehat{u} }
   { y' \left( \widehat{ \Sigma }^{-1} \otimes
   \left( I - \frac{i i'}{T} \right) \right) y }
\end{equation}
where $T$ is the number of observations in each equation,
$I$ is an $T \times T$ identity matrix and 
$i$ is a column vector of $T$ ones.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood ratio test}

Currently, the \code{systemfit} package can perform a Likelihood Ratio
test to test linear restrictions when estimating SUR equation systems.
The function returns the empirical likelihood ratio, the $p$-value and
the degrees of freedom of the test which is equivalent to the number
of restrictions.

% This test calculates the likelihood ratio value by calculating the
% estimated residual covariance matrix $\hat{S}$ by the maximum
% likelihood formula,

% \begin{equation}
% \hat{s}_{ij} = \frac{\hat{e}_i^{'} \hat{e}_j}{T} 
% \end{equation}
% where $T$ is vector of ones. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hausman test}

Hausman \citep{hausman78} developed a test have power against
alternatives for which $\hat{\theta}$ and $\tilde{\theta}$ diverge
under misspecification.  The resulting test statistic, distributed as
a $\chi^2$, is the difference between the covariance of an efficient
estimator and the covariance of the inefficient estimator. If the
difference is zero, the model is correctly specified.

The test is based on the idea that under the null hypothesis of no
correlation, both the limited information estimates and the
full-information results are consistent, but only the full-information
estimates are efficient and under the alternative, the limited
information estimates are consistent, but the full information
estimates are not.

The null hypotheses of the test is that all exogenous variables are
uncorrelated with all disturbance terms.  Under this hypothesis both
the 2SLS and the 3SLS estimator are consistent but only the 3SLS
estimator is (asymptotically) efficient.  Under the alternative
hypothesis the 2SLS estimator is consistent but the 3SLS estimator is
inconsistent.

The Hausman test statistic is,

\begin{equation}
m = ( \beta_2 - \beta_3 )^{'} ( \Omega_2 - \Omega_3 ) ( \beta_2 - \beta_3 ) 
\label{eq:hausman}
\end{equation}
where $\beta_2$ and $\Omega_2$ are the estimated coefficients and the
variance-covariance matrix from a 2SLS estimation and $\beta_3$ and
$\Omega_3$ are the estimated coefficients and their variance
covariance-matrix from a 3SLS estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nonlinear estimation}

As linear systems of equations can be estimated using
\code{systemfit}, nonlinear systems of equations \citep{hausman75} can
be estimated using a similar function; \code{nlsystemfit}.  The usage
for the nonlinear estimation function is similar to \code{systemfit}
and can estimate system of equations using OLS, SUR, 2SLS and 3SLS
methods. The equations are defined as,

\begin{equation}
  \label{eq:non_linear_eq_1}
  \epsilon_{t} = q( y_t, x_t, \beta )
\end{equation}
 
\noindent and

\begin{equation}
  \label{eq:non_linear_eq_2}
  z_{t} = Z( x_t )
\end{equation}
where $\epsilon_{t}$ are the residuals from the $y_t$ observations,
$x_t$ is a matrix of independent variables, $\beta$ is a vector is
parameter estimates and $z_{t}$ are the functions evaluated at the
parameter estimates.

Similar to calling the \code{systemfit} function, \code{nlsystemfit}
is called with a minimum of three arguments,

\code{
R> nlsystemfit( method, eqns, start )
}

where \code{method} is one of the following estimation methods: "OLS",
"SUR", "2SLS", or "3SLS", \code{eqns} is a list of equations similar
to those described for \code{systemfit}, and \code{start} is a list of
starting values of the parameter estimates.  The nlsystemfit function
relies on \code{nlm} to perform the minimization of the objective
functions and the \code{qr} set of functions.  If the user does not
have a set of estimates for the initial parameters, it is suggested
using linearized forms in one of the linear methods, or simply using
\code{nls} to obtain estimates for the equations. The outputs are
similar to those obtained by \code{systemfit} objects.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% left over from the older docs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The mandatory argument
% \code{eqns} is a list of the equations to estimate.  Each equation is
% a standard formula in \proglang{R}.  It starts with a dependent
% variable on the left hand side.  After a tilde ($~$) the regressors
% are listed  \footnote{For Details see the \proglang{R} help files to
%   \code{formula}}.


% The objective functions for the methods are:
  
% \begin{center}
% \begin{tabular}{|l|c|c|c|} \hline
%   Method & Instruments & Objective Function & Covariance of $\theta$ \\ \hline  
%   OLS & No & $r'r$ & $(X(diag(S)^{-1}\otimes I)X)^{-1}$ \\ \hline
%   SUR & No & $r'(diag(S)_{OLS}^{-1}\otimes I)r$ & $(X(S^{-1}\otimes I)X)^{-1}$ \\ \hline
%   2SLS & Yes & $r'(I \otimes W)r$ & $(X(diag(S)^{-1}\otimes I)X)^{-1}$ \\ \hline 
%   3SLS & Yes & $r'(S_{2SLS}^{-1} \otimes W)r$ & $(X(diag(S)^{-1}\otimes W)X)^{-1}$ \\ \hline
% \end{tabular}
% \end{center}

% where, $r$ is a column vector for the residuals for each equation,
% what is the difference to \epsilon?
% $X$ is matrix of the partial derivatives of the dependent variable 
% with respect to the parameters $\left( \frac{ \partial y }{ \theta} \right)$,
% is this correct?
% in the linear section X is used for the regressors, 
% thus we should use a different variable name here.
% $W$ is a matrix of the instrument variables, $Z(Z'Z)^{-1}Z$, $Z$ is a
% matrix of the instrument variables, and $I$ is an $n \times n $
% identity matrix and $S$ is the estimated variance-covariance matrix between the
% equations

% \begin{equation}
%   \label{eq:non-linear_varcov}
%   \hat{\sigma}_{ij} = (\hat{e}_i' \hat{e}_j) / \sqrt{(T - k_i)*(T- k_j)} 
% \end{equation}

% Do we actually have the variance-covariance function in place. 

% The argument \code{method} is a string determining the estimation method.
% It must be one of "OLS", "WLS", "SUR", "2SLS", "W2SLS" or "3SLS".


% \textbf{The residual variance-covariance matrix can be calculated in
%   different ways (section~\ref{sec:residcov}).  It should be
%   relatively easy to implement this also in nlsystemfit(),  e.g. the
%   function 'calcRCov' in systemfit() could be moved outside
%   systemfit() that it can be used also by nlsystemfit().}

% \textbf{You need to clear this up.}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "systemfit"
%%% End: 
