\documentclass[article]{jss}

\author{Arne Henningsen\\University of Kiel\And 
        Jeff D. Hamann\\ ??? }
        
\title{systemfit: A Package to Estimate\\
       Simultaneous Equation Systems in \proglang{R}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Arne Henningsen, Jeff D. Hamann} %% comma-separated
\Plaintitle{systemfit: A Package to Estimate Simultaneous Equation Systems 
            in R} %% without formatting
\Shorttitle{systemfit} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
   Many statistical analysis are based on models that consist of 
   more than one equation.
   These equations should be fitted simultaneously rather than equation 
   by equation.
   The package \pkg{systemfit} provides the capability to estimate systems 
   of linear (and non-linear???) equations in \proglang{R}. 
}
\Keywords{R, equation system, seemingly unrelated regression}
%% at least one keyword must be supplied

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
   Arne Henningsen\\
   Department of Agricultural Economics\\
   University of Kiel\\
   D-24098 Kiel, Germany\\
   E-mail: \email{ahenningsen@agric-econ.uni-kiel.de}\\
   URL: \url{http://www.uni-kiel.de/agrarpol/ahenningsen/}\\
   \\
   Jeff D. Hamann\\
   e-mail: \email{jeff.hamann@forestinformatics.com}\\
   URL: \url{http://www.forestinformatics.com}\\
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many theoretical models consist of more than one equation.
If these models are econometrically estimated, 
the equations generally should not be estimated individually, 
but all together, because the disturbance terms of these 
equations are likely contemporaneously correlated.
Estimating all equations simultaneously and
taking the covariance structure of the residuals into account 
leads to efficient estimates, 
while ignoring this correlation leads to an inefficient 
parameter estimation \citep{zellner62}.

Another reason to estimate an equation system simultaneously are
cross-equation parameter restrictions. These restrictions can 
be tested and/or imposed only in a simultaneous estimation
approach. Especially the economic theory suggests many 
cross-equation restriction.

The \pkg{systemfit} package provides the capability to estimate 
linear (and non-linear???) equation systems in \proglang{R}.
Compared to other statistics and econometrics software 
the (advanced) user can control many important details of the 
estimation procedure. On the other hand these user options have
reasonable defaults that also beginners can easily use the
provided functions.

This paper is organized as follows: 
In section \ref{sec:Theory} we introduce the mathematics of estimating
equation systems and give a comprehensive overview over all
relevant issues regarding this topic.
Section \ref{sec:UsingSystemfit} demonstrates how to run \pkg{systemfit},
especially how the features presented in the previous chapter
can be used.
Finally, a summary and an outlook are given in section \ref{sec:Summmary}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Estimating equation systems}\label{sec:Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Consider a system of $G$ equations, where the $i$th equation is of
the form 
\begin{equation}
   y_{i} = X_i \beta_i + u_i, \quad i = 1, 2, \ldots, G
\end{equation}
where $y_i$ is a vector of the dependent variable,
$X_i$ is a matrix of the exogenous variables,
$\beta_i$ is the coefficient vector and
$u_i$ is a vector of the disturbance terms of the $i$th equation.

We can write the 'stacked' system as
\begin{equation}
   \left[ \begin{array}{c}
      y_1 \\ y_2\\ \vdots\\ y_G
   \end{array} \right] = 
   \left[ \begin{array}{cccc}
      X_1 & 0 & \cdots & 0\\
      0 & X_2 & \cdots & 0\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & X_G
   \end{array}\right] \cdot 
   \left[ \begin{array}{c}
      \beta_1 \\ \beta_2 \\ \vdots\\ \beta_G
   \end{array} \right] +
   \left[ \begin{array}{c}
      u_1 \\ u_2 \\ \vdots\\ u_G
   \end{array} \right]
\end{equation}

or more simply as
\begin{equation}
   y = X \beta + u 
\end{equation}   

We assume that there is no correlation of the disturbance terms 
across observations:
\begin{equation}
   E \left( u_{it} \, u_{jt^*} \right) = 0 
   \; \forall \; t \neq t^*
\end{equation}
where $i$ and $j$ indicate the equation number 
and $t$ and $t^*$ denote the observation number.

However, we explicitly allow for contemporaneous correlation:
\begin{equation}
   E \left( u_{it} \, u_{jt} \right) = \sigma_{ij}
\end{equation}

Thus, the covariance matrix of the total system is
\begin{equation}
   E \left( u \, u' \right) = \Omega = \Sigma \otimes I
\end{equation}
where $\Sigma = \left[ \sigma_{ij} \right]$ and $I$ is an
identity matrix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parameter estimation}

\subsubsection{Ordinary Least Squares estimation}

The Ordinary Least Squares (OLS) estimator of the system 
is obtained by
\begin{equation}
   \widehat{\beta} = \left( X'X \right)^{-1} X'y
\end{equation}

These estimates are efficient only if the disturbance terms are not 
contemporaneously correlated, which means 
\begin{equation}
   \sigma_{ij} = 0 \; \forall \; i \neq j
\end{equation}

If the whole system is treated as one single equation, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \sigma^2 \left( X'X \right)^{-1}
\end{equation}
with $\sigma^2 = E \left( u' u \right)$.
This assumes that the disturbances of all equations have the
same variance.

If the disturbance terms of the individual equations 
are allowed to have different variances, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( X' \Omega^{-1} X \right)^{-1}
   \label{eq:OLSvcm2}
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

If no cross-equation restrictions are imposed, the simultaneous 
OLS estimation of the system leads to the same parameter estimates 
as an equation-wise OLS estimation. The variance-covariance matrix 
of the parameters from an equation-wise OLS estimation is equal to 
the variance-covariance matrix obtained by equation 
(\ref{eq:OLSvcm2}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Weighted least squares}

The Weighted Least Squares (WLS) estimator of the system 
is obtained by
\begin{equation}
   \widehat{\beta} = \left( X' \Omega^{-1} X \right)^{-1} X' \Omega^{-1} y
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

Like the OLS estimates these estimates are only efficient 
if the disturbance terms are not contemporaneously correlated.

The variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( X' \Omega^{-1} X \right)^{-1}
\end{equation}

If no cross-equation restrictions are imposed, 
the parameter estimates and the variance-covariance matrix 
of the parameters are equal to the OLS estimates%
\footnote{The variance-covariance matrices of the 
estimated parameters might differ, 
if the WLS estimation is based on a different formula 
to calculate the (diagonal) variance-covariance 
matrix of the residuals (see section ????).}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Seemingly unrelated regression}

When the disturbances are contemporaneously correlated, a Generalized 
Least Squares (GLS) estimation leads to efficient parameter estimates.
In this case the GLS is generally called "Seemingly Unrelated Regression" 
(SUR) (see \citealp{zellner62}). 
Note, while an unbiased OLS or WLS estimation requires only that 
the regressors and the disturbance terms of each single 
equation are uncorrelated,
a consistent SUR estimation requires that all disturbance terms and all 
regressors are uncorrelated:
\begin{equation}
   E \left( u | X \right) = 0
\end{equation}

The SUR estimator can be obtained by:%
\footnote{To calculate $\Omega^{-1}$ it is not necessary to invert 
the (huge) $\Omega$ matrix, 
because it is sufficient to invert the (small) $\Sigma$ matrix and
calculate it by $\Omega^{-1} = \Sigma^{-1} \otimes I$.}

\begin{equation}
   \widehat{\beta} = \left( X' \Omega^{-1} X \right)^{-1} X' \Omega^{-1} y
\end{equation}
with $\Omega = \Sigma \otimes I$ and
$\sigma_{ij} = E \left( u_i' u_j \right)$.

And the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( X' \Omega^{-1} X \right)^{-1}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Two-stage least squares estimation}

If the regressors of one or more equations are correlated 
with the disturbances ($E \left( u_i | X_i \right) \neq 0$), 
the estimated coefficients are biased.
This can be circumvented by an instrumental variable (IV) estimation.
The instrumental variables for each equation $H_i$ 
can be either different or identical for all equations.
The instrumental variables of each equation may not be correlated with 
the disturbance terms of the corresponding equation 
($E \left( u_i | H_i \right) = 0$).

At the first stage new ('fitted') regressors are obtained by
\begin{equation}
   \widehat{X_i} = H_i \left( H_i' H_i \right)^{-1} H_i' X
\end{equation}

At the second stage the unbiased two-stage least squares estimates
of $\beta$ are obtained by:
\begin{equation}
   \widehat{\beta} = \left( \widehat{X}' \widehat{X} \right)^{-1} 
   \widehat{X}' y 
   \label{eq:beta2sls}
\end{equation}

If the whole system is treated as one single equation, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \sigma^2 \left( \widehat{X}'
   \widehat{X} \right)^{-1}
\end{equation}
with $\sigma^2 = E \left( u' u \right)$.

If the disturbance terms of the individual equations 
are allowed to have different variances, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( \widehat{X}' \Omega^{-1} 
   \widehat{X} \right)^{-1}
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Weighted two-stage least squares estimation}

The Weighted Two-Stage Least Squares (W2SLS) estimator of the system 
is obtained by
\begin{equation}
   \widehat{\beta} = \left( \widehat{X}' \Omega^{-1} \widehat{X} 
   \right)^{-1} \widehat{X}' \Omega^{-1} y
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

The variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( \widehat{X}' \Omega^{-1} 
   \widehat{X} \right)^{-1}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Three-stage least squares estimation}

If the regressors are correlated with the disturbances 
($E \left( u | X \right) \neq 0$) and 
the disturbances are contemporaneously correlated, 
an Instrumental Variable (IV) Generalized Least Squares (GLS) 
estimation leads to consistent and efficient estimates. This estimation 
procedure is generally called "Three-stage Least Squares" (3SLS). 

The standard 3SLS estimator can be obtained by:
\begin{equation}
   \widehat{\beta} = \left( \widehat{X}' \Omega^{-1} \widehat{X} 
   \right)^{-1} \widehat{X}' \Omega^{-1} y
\end{equation}
with $\Omega = \Sigma \otimes I$ and
$\sigma_{ij} = E \left( u_i' u_j \right)$.

Its variance-covariance matrix is:
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( \widehat{X}' \Omega^{-1} 
   \widehat{X} \right)^{-1}
   \label{eq:cov3sls}
\end{equation}

While an unbiased 2SLS or W2SLS estimation requires only that 
the instrumental variables and the disturbance terms of each single 
equation are uncorrelated,
\cite{schmidt90} points out that this estimator is only consistent 
if all disturbance terms and all instrumental variables are uncorrelated:
\begin{equation}
   E \left( u | H \right) = 0
\end{equation}
with
\begin{equation}
   H =
   \left[ \begin{array}{cccc}
      H_1 & 0 & \cdots & 0\\
      0 & H_2 & \cdots & 0\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & H_G
   \end{array}\right] \cdot 
\end{equation}

Since there might be occasions where this cannot be avoided, 
\cite{schmidt90} analyses other approaches to obtain 3SLS estimators:

One of these approaches is based on instrumental variable estimation
(IV-3SLS):
\begin{equation}
   \widehat{\beta} = \left( \widehat{X}' \Omega^{-1} X 
   \right)^{-1} \widehat{X}' \Omega^{-1} y
\end{equation}

The variance-covariance matrix of this IV-3SLS estimator is:
\begin{equation}
   Var \left[ \widehat{\beta} \right] = \left( \widehat{X}' \Omega^{-1} 
   X \right)^{-1}
\end{equation}


Another approach is based on the Generalized Method of Moments (GMM) 
estimator (GMM-3SLS):
\begin{equation}
   \widehat{\beta} = \left( X' H \left( H' \Omega H \right)^{-1}
   H' X \right)^{-1} X' H \left( H' \Omega H \right)^{-1} H' y
   \label{eq:beta3slsGMM}
\end{equation}

The variance-covariance matrix of the GMM-3SLS estimator is:
\begin{equation}
   Var \left[ \widehat{\beta} \right] = 
   \left( X' H \left( H' \Omega H \right)^{-1} H' X \right)^{-1}
\end{equation}


A fourth approach developed by \cite{schmidt90} himself is:
\begin{equation}
   \widehat{\beta} = \left( \widehat{X}' \Omega^{-1} \widehat{X} 
   \right)^{-1} \widehat{X}' \Omega^{-1} 
   H \left( H' H \right)^{-1} H' y 
   \label{eq:beta3slsSchmidt}
\end{equation}

The variance-covariance matrix of this estimator is:
\begin{equation}
   Var \left[ \widehat{\beta} \right] = 
   \left( \widehat{X}' \Omega^{-1}  \widehat{X} \right)^{-1} 
   \widehat{X}' \Omega^{-1} H \left( H' H \right)^{-1} H' \Omega 
   H \left( H' H \right)^{-1} H' \Omega^{-1} \widehat{X}
   \left( \widehat{X}' \Omega^{-1}  \widehat{X} \right)^{-1}
\end{equation}


The econometrics software EViews uses following approach:
\begin{equation}
   \widehat{\beta} = \widehat{\beta}_{2SLS} + 
   \left( \widehat{X}' \Omega^{-1} \widehat{X} \right)^{-1} 
   \widehat{X}' \Omega^{-1} \left( y - X \widehat{\beta}_{2SLS} \right)
\end{equation}
with $\widehat{\beta}_{2SLS}$ is the two-stage least squares estimator
(equation \ref{eq:beta2sls}).
EViews uses the standard 3SLS formula (equation (\ref{eq:cov3sls})) to 
calculate the variance-covariance matrix of the 3SLS estimator.


If the same instrumental variables are used in all equations 
($H_1 = H_2 = \ldots = H_G$), 
all the above mentioned approaches lead to identical parameter estimates.
However, if this is not the case, the results depend on the 
equation used \citep{schmidt90}. 
The only reason to use different instruments in the different equations 
is that the instruments of one equation are correlated with the
disturbance terms in another equation. Otherwise, one would simply use
every instrument in every equation \citep{schmidt90}. 
In this case, only the GMM-3SLS (equation (\ref{eq:beta3slsGMM}))
and the 3SLS estimator developed by \cite{schmidt90} 
(equation (\ref{eq:beta3slsSchmidt})) are consistent.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imposing linear restrictions}
There are two ways to impose linear parameter restrictions.
First, a matrix $T$ can be provided with
\begin{equation}
   \beta = T \cdot \beta^* \label{eq:T-restr} 
\end{equation}
where $\beta^*$ is a vector of linear independent coefficients.

To impose these restrictions the $X$ matrix is
(post-)multiplied by this $T$ matrix.
\begin{equation}
    X^* = X \cdot T
\end{equation}

Then, a standard estimation is done by substituting $X^*$ for $X$.
This results in the linear independent parameter estimates $\beta^*$ and
the variance-covariance matrix of these parameters. 
The original parameters can be obtained by equation (\ref{eq:T-restr})
and the variance-covariance matrix of the original parameters 
can be obtained by:
\begin{equation}
   Var \left[ \widehat{\beta} \right] = T \cdot Var \left[ \widehat{\beta^*} \right] \cdot T'
\end{equation}

The second way to impose linear parameter restrictions 
can be formulated by
\begin{equation}
   R \beta^0 = q
\end{equation}
where $\beta^0$ is the vector of the restricted coefficients, 
$R$ is a matrix and $q$ is a vector (see \citealp[p. 100]{greene02}). 
Each restriction belongs to one row of $R$ and the corresponding 
element of~$q$.

The first way is less flexible than this latter one%
\footnote{e.g. restrictions like $\beta_1 + \beta_2 = 4$ cannot be imposed
by the first method}, 
but the first way is preferable if equality constraints for coefficients
across many equations of the system are imposed. 
Of course, these restrictions can be also imposed using
the latter method.
However, while the latter method increases the dimension of the 
matrices to be inverted during estimation, the first reduces it. 
Thus, in some cases the latter way leads to estimation problems
(e.g. (near) singularity of the matrices to be inverted),
while the first doesn't.

These two methods can be combined. In this case the restrictions
imposed using the latter method are imposed on the linear independent 
parameters due to the restrictions imposed using the first method:
\begin{equation}
   R \beta^{*0} = q
\end{equation}
where $\beta^{*0}$ is the vector of the restricted $\beta^*$ coefficients.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted OLS estimation}

The OLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      X' X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      X' y \\ q 
   \end{array} \right]
\end{equation}
where $2\lambda$ is a vector of the Lagrangean multipliers of the restrictions.

If the whole system is treated as one single equation, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = \sigma^2 
   \left[ \begin{array}{cc}
      X' X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}
with $\sigma^2 = E \left( u' u \right)$.

If the disturbance terms of the individual equations 
are allowed to have different variances, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      X' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted WLS estimation}

The WLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      X' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      X' \Omega^{-1} y \\ q 
   \end{array} \right]
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

The variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      X' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted SUR estimation}

The SUR estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      X' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      X' \Omega^{-1} y \\ q 
   \end{array} \right]
\end{equation}
with $\Omega = \Sigma \otimes I$ and
$\sigma_{ij} = E \left( u_i' u_j \right)$.

The variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      X' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted 2SLS estimation}

The 2SLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' y \\ q 
   \end{array} \right]
   \label{eq:beta2SLSr}
\end{equation}

If the whole system is treated as one single equation, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = \sigma^2 
   \left[ \begin{array}{cc}
      \widehat{X}' \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}
with $\sigma^2 = E \left( u' u \right)$.

If the disturbance terms of the individual equations 
are allowed to have different variances, 
the variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted W2SLS estimation}

The W2SLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' \Omega^{-1} y \\ q 
   \end{array} \right]
\end{equation}
with $\Omega = \Sigma \otimes I$, 
$\sigma_{ij} = 0 \; \forall \; i \neq j$ and
$\sigma_{ii} = E \left( u_i' u_i \right)$.

The variance-covariance matrix of the estimated parameters is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restricted 3SLS estimation}

The standard 3SLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' \Omega^{-1} y \\ q 
   \end{array} \right]
\end{equation}
with $\Omega = \Sigma \otimes I$ and
$\sigma_{ij} = E \left( u_i' u_j \right)$.

The variance-covariance matrix of this estimator is
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \label{eq:cov3slsr}
\end{equation}


The IV-3SLS estimator restricted by $R \beta^0 = q$ can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' \Omega^{-1} y \\ q 
   \end{array} \right]
\end{equation}

with
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}


The restricted 3SLS-GMM estimator can be obtained by
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      X' H \left( H' \Omega H \right)^{-1} H' X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      X' H \left( H \Omega H \right)^{-1} H' y \\ q 
   \end{array} \right]
\end{equation}

with
\begin{equation}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right] 
   = 
   \left[ \begin{array}{cc}
      X' H \left( H' \Omega H \right)^{-1} H' X & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{equation}


The restricted 3SLS estimator based on the suggestion of 
\cite{schmidt90} is:
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' \Omega^{-1} H \left( H' H \right)^{-1} H' y \\ q 
   \end{array} \right]
\end{equation}

with
\begin{eqnarray}
   Var 
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right] 
   & = & 
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} H \left( H' H \right)^{-1} H' \Omega
      H \left( H' H \right)^{-1} H' \Omega^{-1} \widehat{X} & 0' \\ 
      0 & 0
   \end{array} \right]^{-1}
   \nonumber \\
   & & \cdot
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
\end{eqnarray}


The econometrics software EViews calculates the restricted 3SLS estimator by:
\begin{equation}
   \left[ \begin{array}{c}
      \widehat{\beta^0} \\ \widehat{\lambda}
   \end{array} \right]
   =
   \left[ \begin{array}{cc}
      \widehat{X}' \Omega^{-1} \widehat{X} & R' \\ 
      R & 0
   \end{array} \right]^{-1}
   \cdot
   \left[ \begin{array}{c}
      \widehat{X}' \Omega^{-1} \left( y - X \widehat{\beta^0}_{2SLS} \right) 
      \\ q 
   \end{array} \right]
\end{equation}
where $\widehat{\beta^0}_{2SLS}$ is the restricted 2SLS estimator calculated
by equation (\ref{eq:beta2SLSr}). 
To calculate the variance-covariance matrix 
EViews uses the standard formula of the restricted 3SLS estimator
(equation (\ref{eq:cov3slsr})).


If the same instrumental variables are used in all equations 
($H_1 = H_2 = \ldots = H_G$), 
all the above mentioned approaches lead to identical parameter estimates
and identical variance-covariance matrices of the estimated parameters.

(***** This has to be checked!!!!! **************)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other issues}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Residual variance-covariance matrix}

Since the true residuals of the estimated equations are generally not known,
the true covariance matrix of the residuals cannot be determined.
Thus, the this covariance matrix must be calculated from the
\emph{estimated} residuals. 

The estimated covariance matrix of the residuals ($\widehat{\Sigma} = 
\left[ \sigma \right]$) 
is often calculated by
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }{ T }
   \label{eq:rcov0}
\end{equation}
where $T$ is the number of observations in each equation.

However, this estimator is biased, because it is not corrected for the
degrees of freedom. 
The usual single-equation procedure to correct for degrees of freedom 
cannot always be applied, because the number of regressors in each equation 
might differ.
Two alternative approaches to calculate the residual variance-covariance 
matrix are
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { \sqrt{ \left( T - K_i \right) \cdot \left( T - K_j \right) } }
   \label{eq:rcov1}
\end{equation}
and
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j }
   { T - \max \left( K_i , K_j \right) }
\end{equation}
where $K_i$ and $K_j$ are the number of regressors in equation
$i$ and $j$, respectively.

However, this yields an unbiased estimator only if $K_i = K_j$ 
\citep[p. 469]{judge85}. 
% Greene (2002, p. 344) says that the second is unbiased if i = j or K_i = K_j,
% whereas the first is unbiased only if i = j. 
% However, if K_i = K_j the first and the second are equal.
% Why is the first biased if K_i = K_j ???????????


A further approach to obtain the estimated residual variance-covariance
matrix is
\begin{equation}
   \widehat{\sigma}_{ij} = \frac{ \widehat{u}_i' \widehat{u}_j } 
   { T - K_i - K_j + tr \left[ \left( X_i' X_i \right)^{-1}
   X_i' X_j \left( X_j' X_j \right)^{-1} X_j' X_i \right] }
   \label{eq:rcov2}
\end{equation} 
This yields an unbiased estimator for all elements of $\widehat{\Sigma}$,
but even if $\widehat{\Sigma}$ is an unbiased estimator of $\Sigma$, 
its inverse $\widehat{\Sigma}^{-1}$ is not an unbiased estimator 
of $\Sigma^{-1}$ \citep[p.322]{theil71}.
Furthermore, the variance-covariance matrix calculated by (\ref{eq:rcov2})
is not necessarily positive semidefinite \citep[p.322]{theil71}. 
Hence, "it is doubtful whether [this formula] is really superior to 
[equation (\ref{eq:rcov0})]" \citep[p.322]{theil71}.


The WLS, SUR, W2SLS and 3SLS parameter estimates are consistent,
if the estimated residual variance-covariance matrix is calculated
using the residuals from a first step OLS or 2SLS estimation.
It is also possible to iterate the estimation procedure and 
calculate the residual variance-covariance matrix from the
residuals of the previous iteration. 
If equation (\ref{eq:rcov0}) is applied to calculate the estimated
residual variance-covariance matrix, the results are maximum
likelihood \citep[p. 345]{greene02}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Degrees of freedom}

There exist different approaches to determine the degrees of
freedom for testing the estimated parameters with a t-test.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Goodness of fit}

The goodness of fit of each equation can be measured by the 
traditional $R^2$ values:
\begin{equation}
   R_i^2 = 1 - \frac{ \widehat{u}_i' \widehat{u}_i }
   { ( y_i - \overline{y_i} )' ( y_i - \overline{y_i} ) }
\end{equation}
where $R_i^2$ is the $R^2$ value of the $i$th equation
and $\overline{y_i}$ is the mean value of $y_i$.

The goodness of fit of the whole system can be measured by the
McElroy's $R^2$ value \citep{mcelroy77}: 
% also: \citep[p. 345]{greene02}
\begin{equation}
   R_*^2 = 1 - \frac{ \widehat{u}' \widehat{ \Omega }^{-1} \widehat{u} }
   { y' \left( \widehat{ \Sigma }^{-1} \otimes
   \left( I - \frac{i i'}{T} \right) \right) y }
\end{equation}
where $T$ is the number of observations in each equation,
$I$ is an $T \times T$ identity matrix and 
$i$ is a column vector of $T$ ones.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using systemfit}\label{sec:UsingSystemfit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Standard usage}

\pkg{systemfit} is generally called by

\code{
R> systemfit( method, eqns )
}

There are two mandatory arguments: \code{method} and \code{eqns}.

The argument \code{method} is a string determining the estimation method.
It must be one of "OLS", "WLS", "SUR", "2SLS", "W2SLS" or "3SLS".

The other mandatory argument \code{eqns} is a list of the equations 
to estimate. 
Each equation is a standard formula in \proglang{R}.
It starts with a dependent variable on the left hand side.
After a tilde ($~$) the regressors are listed%
\footnote{For Details see the \proglang{R} help files to \code{formula}}.

This is now demonstrated using an example: \\
\code{
R> library( systemfit ) \\
R> data( kmenta ) \\
R> attach( kmenta ) \\
R> fitsur <- systemfit( "SUR", list( q $\sim$ p + d, q $\sim$ p + f + a ) ) \\
}

The first line loads the \pkg{systemfit} package. 
The second line loads example data that are included in this package.
These data come from \cite{kmenta86}.
They are attached to the \proglang{R} search path in line three.
In the last line a seemingly unrelated regression is done.
The first equation represents the demand side of the food market.
The dependant variable is \code{q} (food consumption per capita). 
The regressors are \code{p} (ratio of food prices to general consumer prices)
and \code{disposable income} as well as a constant%
\footnote{a regression constant is always implied if not explicitly omitted.}.
The second equation represents the supply side.
Variable \code{q} (food consumption per capita) is also the dependant 
variable of this equation. 
The regressors are again \code{p} (ratio of food prices to general 
consumer prices) and a constant as well as 
\code{f} (ratio of preceding year's prices received by farmers) and 
\code{a} (a time trend in years).
The regression result is assigned to the variable \code{fitsur}.

Summary results can be printed by\\
\code{
R>~summary(~fitsur~)~\\
~\\
systemfit~results~\\
method:~SUR~\\
\\
\textcolor{white}{.}~~N~DF~~~~~~SSR~~~~~MSE~~~~RMSE~~~~~~~R2~~~Adj~R2 \\
1~20~17~~65.6829~3.86370~1.96563~0.755019~0.726198~\\
2~20~16~104.0584~6.50365~2.55023~0.611888~0.539117~\\
~\\
The~covariance~matrix~of~the~residuals~used~for~estimation\\
\textcolor{white}{.}~~~~~~~1~~~~~~~2~\\
1~3.72539~4.13696~\\
2~4.13696~5.78444~\\
~\\
The~covariance~matrix~of~the~residuals\\
\textcolor{white}{.}~~~~~~~1~~~~~~~2~\\
1~3.86370~4.92431~\\
2~4.92431~6.50365~\\
~\\
The~correlations~of~the~residuals\\
\textcolor{white}{.}~~~~~~~~1~~~~~~~~2~\\
1~1.000000~0.982348~\\
2~0.982348~1.000000~\\
~\\
The~determinant~of~the~residual~covariance~matrix:~0.879285~\\
OLS~R-squared~value~of~the~system:~0.683453~\\
McElroy's~R-squared~value~for~the~system:~0.788722~\\
~\\
SUR~estimates~for~1~~(equation~1~)~\\
Model~Formula:~q~~~p~+~d\\
\\
\textcolor{white}{.}~~~~~~~~~~~~Estimate~Std.~Error~~~t~value~Pr(>|t|)~\\
(Intercept)~99.332894~~~7.514452~13.218913~~~~~~~~0~***~\\
p~~~~~~~~~~~-0.275486~~~0.088509~-3.112513~0.006332~~**~\\
d~~~~~~~~~~~~~0.29855~~~0.041945~~7.117605~~~~2e-06~***~\\
---~\\
Signif.~codes:~~0~`***'~0.001~`**'~0.01~`*'~0.05~`.'~0.1~`~'~1~\\
~\\
Residual~standard~error:~1.96563~on~17~degrees~of~freedom~\\
Number~of~observations:~20~Degrees~of~Freedom:~17~\\
SSR:~65.682902~MSE:~3.8637~Root~MSE:~1.96563~\\
Multiple~R-Squared:~0.755019~Adjusted~R-Squared:~0.726198~\\
~\\
~\\
SUR~estimates~for~2~~(equation~2~)~\\
Model~Formula:~q~~~p~+~f~+~a\\
\\
\textcolor{white}{.}~~~~~~~~~~~~Estimate~Std.~Error~~t~value~Pr(>|t|)~\\
(Intercept)~61.966166~~~11.08079~5.592215~~~~4e-05~***~\\
p~~~~~~~~~~~~0.146884~~~0.094435~1.555397~0.139408~\\
f~~~~~~~~~~~~0.214004~~~0.039868~5.367761~~6.3e-05~***~\\
a~~~~~~~~~~~~0.339304~~~0.067911~4.996283~0.000132~***~\\
---~\\
Signif.~codes:~~0~`***'~0.001~`**'~0.01~`*'~0.05~`.'~0.1~`~'~1~\\
~\\
Residual~standard~error:~2.550226~on~16~degrees~of~freedom~\\
Number~of~observations:~20~Degrees~of~Freedom:~16~\\
SSR:~104.05843~MSE:~6.503652~Root~MSE:~2.550226~\\
Multiple~R-Squared:~0.611888~Adjusted~R-Squared:~0.539117~\\
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{User options}

Following additional options can be set by the user:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Equation labels}
The optional argument \code{eqnlabels} allows the user to label the equations.
It has to be a vector of strings naming the equations.\\
\code{
R>~fitsur~<-~systemfit(~"SUR",~list(~q~~~p~+~d,~q~~~p~+~f~+~a~),\\~
R+~~~~eqnlabels~=~c(~"demand",~"supply"~)~)\\
R>~summary(~fitsur~)\\
systemfit~results\\
method:~SUR\\
\\
\textcolor{white}{.}~~~~~~~N~DF~~~~~~SSR~~~~~MSE~~~~RMSE~~~~~~~R2~~~Adj~R2\\
demand~20~17~~65.6829~3.86370~1.96563~0.755019~0.726198\\
supply~20~16~104.0584~6.50365~2.55023~0.611888~0.539117\\
\ldots\\
}
If no equation labels are provided, the equations are numbered.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Instrumental variables}   
\code{inst}one-sided model formula specifying instrumental variables
   or a list of one-sided model formulas if different instruments should
   be used for the different equations (only needed for 2SLS, W2SLS and
   3SLS estimations).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Data}   
\code{data} an optional data frame containing the variables in the model.
   By default the variables are taken from the environment from which
   systemfit is called.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Restrictions}   
\code{R.restr} an optional j x k matrix to impose linear
   restrictions on the parameters by \code{R.restr} * $\beta$ = \code{q.restr}
   (j = number of restrictions, k = number of all parameters,
   $\beta$ = vector of all parameters).

\code{q.restr} an optional j x 1 matrix to impose linear
   restrictions (see \code{R.restr}); default is a j x 1 matrix
   that contains only zeros.

\code{TX} an optional matrix to transform the regressor matrix and,
   hence, also the coefficient vector (see details).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Iteration control}   
\code{maxiter} maximum number of iterations for WLS, SUR, W2SLS and
   3SLS estimations.

\code{tol} tolerance level indicating when to stop the iteration (only
   WLS, SUR, W2SLS and 3SLS estimations).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Residual covariance matrix}   
\code{rcovformula} formula to calculate the estimated residual covariance
   matrix (see details).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{3SLS formula}   
\code{formula3sls} formula for calculating the 3SLS estimator,
   one of "GLS", "IV", "GMM", "Schmidt" or "EViews" (see details).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Degrees of freedom for t-tests}   
\code{probdfsys} use the degrees of freedom of the whole system
   (in place of the degrees of freedom of the single equation)
   to calculate prob values for the t-test of individual parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sigma squared}   
\code{single.eq.sigma} use different $\sigma^2$s for each
   single equation to calculate the covariance matrix and the
   standard errors of the coefficients (only OLS and 2SLS).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{System options}   
\code{solvetol} tolerance level for detecting linear dependencies
   when inverting a matrix or calculating a determinant (see
   \code{solve} and \code{det}).

\code{saveMemory} save memory by omitting some calculation that
   are not crucial for the basic estimation (e.g McElroy's
   $R^2$).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other tools}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Likelihood ratio test}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hausman test}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and outlook}\label{sec:Summmary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

not implemented yet: unequal numbers of observations

nlsystemfit finished?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{/home/suapm095/Documents/Literatur/arne}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
